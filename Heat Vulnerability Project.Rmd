---
title: "MGT 286B - Capstone project -  Classifing Heat Vulnerability in California
  ZIP Codes - Group 11"
author: "Aakash Nagare, Siddharth Patil, Reva Arora, Kavin Devraj"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document:
    df_print: paged
---
***Research Questions and Objectives***

**Research Questions**
*1. Can we classify ZIP codes by heat vulnerability using socio-environmental indicators?*
*2. What are the most influential variables?*
*3. How can this inform targeted policy?*

**Objectives**
*- Train and evaluate ML models*
*- Rank influential factors*
*- Map ZIP-level heat vulnerability*

```{r setup, include=FALSE} 
# Set CRAN mirror to avoid "no mirror set" errors 
options(repos = c(CRAN = "https://cran.rstudio.com")) # Set global chunk options 
knitr::opts_chunk$set(echo = TRUE) 
```

*Load packages*
```{r}
library(dplyr)
library(tidyr)
library(readxl)
library(ggplot2)
library(caret)
library(randomForest)
library(rpart)
library(rpart.plot)
library(nnet)
library(pROC)
library(corrplot)
library(gridExtra)
library(sf)
library(tigris)
library(patchwork)
library(ggrepel)
library(pdp)
set.seed(286)
```

***1. Load and Clean Data***
```{r}
df <- read_excel("HHI_Data_2024_United_States.xlsx") %>%
  filter(STATE == "CA") %>%
  select(STATE, ZCTA, OVERALL_RANK,
         PR_POV, PR_UNINSUR, PR_NOHSDP, PR_ELP, PR_DISABL, PR_ODW,
         F_OBS, F_ASTHMA, F_DIABETES, F_COPD,
         PR_TREEC, PR_IMPERV, PR_MOBILE, PR_NOVEH, PR_RENT,
         PR_PM25, PR_OZONE) %>%
  mutate(across(where(is.numeric), ~na_if(., -999))) %>%
  drop_na() %>%
  mutate(HIGH_RISK = as.factor(ifelse(OVERALL_RANK >= 0.75, 1, 0)))

```
**Loaded the dataset, filtered only California ZIP codes, removed invalid entries, and created a binary target variable HIGH_RISK indicating top 25% most vulnerable ZIPs.**

***2. Exploratory Data Analysis***

```{r}
ggplot(df, aes(x = factor(HIGH_RISK), y = PR_TREEC)) +
  geom_violin(fill = "lightgreen") +
  labs(title = "Tree Canopy Percentile by Heat Risk", x = "High Risk", y = "Tree Canopy") +
  theme_minimal()
```
**Visualized relationships between variables and heat risk.**
**Violin plot: Less tree canopy in high-risk ZIPs.**

*Density plot for PM2.5 pollution percentile across ZIP codes*
```{r}
ggplot(df, aes(x = PR_PM25)) +
  geom_density(fill = "steelblue", alpha = 0.6) +
  labs(
    title = "Distribution of PM2.5 Pollution Across California ZIP Codes",
    x = "PM2.5 Pollution Percentile",
    y = "Density"
  ) +
  theme_minimal(base_size = 13)
```
**Showed distribution of PM2.5 air pollution scores. Some ZIPs have significantly higher exposure.**

***Boxplot of uninsured population percentiles grouped by heat risk***
```{r}
ggplot(df, aes(x = factor(HIGH_RISK), y = PR_UNINSUR, fill = factor(HIGH_RISK))) +
  geom_boxplot(alpha = 0.7) +
  scale_fill_manual(values = c("0" = "gray70", "1" = "firebrick"), name = "High Risk") +
  labs(
    title = "Uninsured Population Percentile by Heat Risk Category",
    x = "Heat Risk (0 = No, 1 = Yes)",
    y = "Percentile of Uninsured Residents"
  ) +
  theme_minimal(base_size = 13)
```
**High-risk ZIP codes tend to have higher percentages of uninsured people, suggesting vulnerability due to healthcare inaccessibility.**

*Class Distribution*
```{r}
class_dist <- table(df$HIGH_RISK)
print("Class Distribution:")
print(class_dist)
print(paste0("Percentage of high-risk ZIP codes: ", round(class_dist[2]/sum(class_dist)*100, 2), "%"))
# Correlation matrix of numeric predictors
numeric_vars <- df %>% 
  select(PR_POV, PR_UNINSUR, PR_NOHSDP, PR_ELP, PR_DISABL, PR_ODW,
         PR_TREEC, PR_IMPERV, PR_MOBILE, PR_NOVEH, PR_RENT,
         PR_PM25, PR_OZONE)

cor_matrix <- cor(numeric_vars, use = "complete.obs")

# Correlation heatmap
corrplot(cor_matrix, method = "color", type = "upper", tl.cex = 0.7,
         title = "Correlation Heatmap of Predictors")
```
**Printed the class balance — about 40.5% of ZIP codes are high-risk. Important for model accuracy metrics.**

***3. Split Data***
```{r}
X <- df %>% select(-STATE, -ZCTA, -OVERALL_RANK, -HIGH_RISK)
y <- df$HIGH_RISK
train_idx <- createDataPartition(y, p = 0.7, list = FALSE)
X_train <- X[train_idx, ]; X_test <- X[-train_idx, ]
y_train <- y[train_idx]; y_test <- y[-train_idx]
```
**Split the cleaned dataset into training (70%) and testing (30%) sets for model evaluation. This prevents overfitting.**

***4. Random Forest***
```{r}
rf_model <- randomForest(X_train, y_train, ntree = 500, importance = TRUE)
rf_pred <- predict(rf_model, X_test)
rf_prob <- predict(rf_model, X_test, type = "prob")[,2]
rf_conf <- confusionMatrix(rf_pred, y_test)
rf_roc <- roc(as.numeric(y_test) - 1, rf_prob)
rf_auc <- auc(rf_roc)
rf_conf
rf_auc

```

**The Random Forest model is an ensemble learning method that builds multiple decision trees on random subsets of data and features, and then averages their predictions. This reduces overfitting and improves generalization.**

**In this project, it achieved an accuracy of approximately 90%, meaning it correctly classified most ZIP codes as either high-risk or not based on the provided features.**

**The model's AUC (Area Under the ROC Curve) was 0.965, which is considered excellent. This means the model is highly effective at distinguishing between high-risk and low-risk ZIP codes — a score close to 1.0 indicates near-perfect classification.**

**The F1 Score was also high, showing a good balance between precision (avoiding false positives) and recall (capturing actual high-risk ZIP codes). This is crucial for real-world policy applications where both over-alerting and under-detection have consequences.**

**Random Forest performed the best among all models tested because:**

**It naturally handles complex, nonlinear interactions between variables.**
**It is robust to missing values and outliers.**
**It offers internal metrics like Gini importance, which helps in ranking the most influential predictors.**

**Feature Importance (Random Forest)**
```{r}
rf_imp_df <- data.frame(Feature = rownames(rf_model$importance),
                        MeanDecreaseGini = rf_model$importance[, "MeanDecreaseGini"])
ggplot(rf_imp_df[1:10, ], aes(x = reorder(Feature, MeanDecreaseGini), y = MeanDecreaseGini)) +
  geom_bar(stat = "identity", fill = "steelblue") + coord_flip() +
  labs(title = "Random Forest Feature Importance", x = "Feature", y = "Mean Decrease in Gini") +
  theme_minimal()
```

**Plotted top 10 features based on Mean Decrease in Gini. Visual proof of predictor strength.**
**According to the model, the most important features contributing to heat vulnerability were:**

**PR_POV: Poverty percentile — high poverty is strongly linked with vulnerability.**
**F_DIABETES: Prevalence of diabetes — a marker of poor health infrastructure.**
**PR_NOHSDP: Percent without a high school diploma — education disparity.**
**PR_ODW: Older adults living alone — a key demographic at risk during heat events.**

***5. Decision Tree***

```{r}
tree_model <- rpart(HIGH_RISK ~ ., data = cbind(X_train, HIGH_RISK = y_train), method = "class")
tree_pred <- predict(tree_model, X_test, type = "class")
tree_prob <- predict(tree_model, X_test)[,2]
tree_conf <- confusionMatrix(tree_pred, y_test)
tree_roc <- roc(as.numeric(y_test) - 1, tree_prob)
tree_auc <- auc(tree_roc)
tree_conf
tree_auc
rpart.plot(tree_model)

```

**The Decision Tree model builds a single, interpretable tree structure that splits the dataset based on the most informative features at each step. Each internal node tests a feature, and each branch represents an outcome of the test, leading to a final classification.**

**In this project, the model achieved an accuracy of around 84%, meaning it correctly predicted the heat risk classification for the majority of ZIP codes, but was less accurate than Random Forest.**

**The AUC (Area Under the ROC Curve) was 0.869, which is considered good. It indicates that the model has a decent ability to distinguish between high-risk and low-risk areas, although it performs noticeably weaker than the Random Forest model.**

**The F1 Score was moderate, reflecting a fair trade-off between precision and recall. It means that while the model captured some high-risk ZIPs well, it also made more misclassifications compared to other models.**

**Advantages of the Decision Tree:**

**Interpretability: It produces a clear and visual flowchart that is easy for policymakers and stakeholders to understand.**
**Transparency: You can directly see how variables like poverty or education are used to classify heat risk.**

**Limitations:**

**Overfitting: It may capture noise in the training data, reducing generalization to unseen data.**

**Instability: Small changes in the data can lead to a completely different tree structure.**

**Less robust to missing values or noisy features compared to ensemble models like Random Forest..**

*Feature Importance (Decision Tree)*
```{r}
tree_imp_df <- data.frame(Feature = names(tree_model$variable.importance),
                          Importance = tree_model$variable.importance)
ggplot(tree_imp_df[1:10, ], aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "darkorange") + coord_flip() +
  labs(title = "Decision Tree Feature Importance", x = "Feature", y = "Importance") +
  theme_minimal()
```
**Showed feature ranking based on node purity in tree structure.**

***6. Neural Network***
```{r}
preproc <- preProcess(X_train, method = c("center", "scale"))
X_train_scaled <- predict(preproc, X_train)
X_test_scaled <- predict(preproc, X_test)
y_train_numeric <- class.ind(y_train)
nn_model <- nnet(x = X_train_scaled, y = y_train_numeric, size = 5, softmax = TRUE, maxit = 200, trace = FALSE)
nn_prob <- predict(nn_model, X_test_scaled)[,2]
nn_pred <- as.factor(ifelse(nn_prob > 0.5, 1, 0))
nn_conf <- confusionMatrix(nn_pred, y_test)
nn_roc <- roc(as.numeric(y_test) - 1, nn_prob)
nn_auc <- auc(nn_roc)
nn_conf
nn_auc
```
```{r install_and_load_diagrammer, message=FALSE}
# Install DiagrammeR only if not already installed
if (!requireNamespace("DiagrammeR", quietly = TRUE)) {
  install.packages("DiagrammeR")
}
library(DiagrammeR)

grViz("
digraph neural_net {

  # Graph layout
  rankdir=LR;
  node [shape = circle, fixedsize=true, width=0.9, fontname = Helvetica];

  # Input layer (17 nodes, we name 3 for illustration)
  subgraph cluster_input {
    label = 'Input Layer (17 features)';
    style = dashed;
    I1 [label='PR_POV'];
    I2 [label='F_DIABETES'];
    I3 [label='PR_UNINSUR'];
    I4 [label='...'];
    I5 [label='I5'];
    I6 [label='I6'];
    I7 [label='I7'];
    I8 [label='I8'];
    I9 [label='I9'];
    I10 [label='I10'];
    I11 [label='I11'];
    I12 [label='I12'];
    I13 [label='I13'];
    I14 [label='I14'];
    I15 [label='I15'];
    I16 [label='I16'];
    I17 [label='I17'];
  }

  # Hidden layer (5 nodes)
  subgraph cluster_hidden {
    label = 'Hidden Layer (5 nodes)';
    style = dashed;
    H1; H2; H3; H4; H5;
  }

  # Output layer (2 classes)
  subgraph cluster_output {
    label = 'Output Layer';
    style = dashed;
    O1 [label='Class 0'];
    O2 [label='Class 1'];
  }

  # Connect inputs to hidden layer
  I1 -> H1; I1 -> H2; I1 -> H3; I1 -> H4; I1 -> H5;
  I2 -> H1; I2 -> H2; I2 -> H3; I2 -> H4; I2 -> H5;
  I3 -> H1; I3 -> H2; I3 -> H3; I3 -> H4; I3 -> H5;
  I4 -> H1; I4 -> H2; I4 -> H3; I4 -> H4; I4 -> H5;
  I5 -> H1; I6 -> H2; I7 -> H3; I8 -> H4; I9 -> H5;
  I10 -> H1; I11 -> H2; I12 -> H3; I13 -> H4; I14 -> H5;
  I15 -> H1; I16 -> H2; I17 -> H3;

  # Connect hidden to output layer
  H1 -> O1; H1 -> O2;
  H2 -> O1; H2 -> O2;
  H3 -> O1; H3 -> O2;
  H4 -> O1; H4 -> O2;
  H5 -> O1; H5 -> O2;
}
")
```
**Input Layer: 17 features (e.g., PR_POV, F_DIABETES, PR_UNINSUR), each representing socioeconomic or health indicators.**

**Hidden Layer: 5 neurons (H1–H5) that learn patterns and interactions across variables — such as the combined effect of poverty and chronic illness.**

**Output Layer: 2 nodes — predicting Class 0 (low-risk) or Class 1 (high-risk) ZIP codes.**

**Connections: Every input node is connected to all hidden neurons, and every hidden node connects to both output classes — allowing the model to learn complex, nonlinear relationships.**

**The Neural Network model consists of layers of interconnected "neurons" that process inputs through mathematical functions to capture complex, nonlinear relationships between features and the target variable. It is a flexible and powerful machine learning method.**

**In this project, the neural network achieved an accuracy of about 86%, which is strong — better than the Decision Tree and only slightly below the Random Forest model.**

**The AUC (Area Under the ROC Curve) was 0.905, which is very good. This indicates that the model can distinguish well between high-risk and low-risk ZIP codes, even in cases where relationships between variables are not linear or obvious.**

**The F1 Score was also high, demonstrating a solid balance between precision and recall. This means the model is effective both at identifying truly high-risk ZIP codes and avoiding false alarms.**

**Advantages:**

**Captures nonlinear relationships that simpler models may miss.**
**Performs well with standardized or scaled data, especially when relationships between features are subtle or interact in complex ways.**
**Can generalize well to new data when tuned properly.**

**Disadvantages:**

**Hard to interpret: Unlike decision trees or feature importance plots, neural networks operate as a "black box" and don’t provide easy visual explanations.**
**Requires careful preprocessing like scaling and sometimes more hyperparameter tuning (e.g., number of layers, neurons, epochs).**

**Feature Weights Insight:**
**The model’s input weights revealed that the most influential features included:**
**PR_POV (Poverty percentile)**
**F_DIABETES (Diabetes prevalence)**
**PR_UNINSUR (Uninsured population percentile)**
**These are consistent with results from Random Forest and Decision Tree, confirming their significance.**

*Feature Importance (NN Weights)*
```{r}
nn_wts <- data.frame(Feature = colnames(X_train_scaled),
                     Weight = abs(nn_model$wts[1:length(colnames(X_train_scaled))]))
ggplot(nn_wts[1:10, ], aes(x = reorder(Feature, Weight), y = Weight)) +
  geom_bar(stat = "identity", fill = "purple") + coord_flip() +
  labs(title = "Neural Network Input Weights (Top 10)", x = "Feature", y = "Weight") +
  theme_minimal()

```
**Plotted top 10 features from the neural net input weights. Overlaps with RF (e.g., diabetes, uninsured).**

***7. Compare ROC Curves***

```{r}
plot(rf_roc, col = "blue", main = "ROC Curve Comparison", lwd = 2)
plot(tree_roc, col = "darkorange", add = TRUE, lwd = 2)
plot(nn_roc, col = "purple", add = TRUE, lwd = 2)
legend("bottomright", legend = c("Random Forest", "Decision Tree", "Neural Net"),
       col = c("blue", "darkorange", "purple"), lwd = 2)

```
**Plotted ROC curves for all three models.**
**Random Forest performed best**
**Followed by Neural Network and Decision Tree**

***8. Feature Importance Comparison***

```{r}
rf_imp <- data.frame(Feature = rownames(rf_model$importance),
                     Importance = rf_model$importance[, "MeanDecreaseGini"],
                     Model = "Random Forest")

nn_wts <- data.frame(Feature = colnames(X_train_scaled),
                     Importance = abs(nn_model$wts[1:length(colnames(X_train_scaled))]),
                     Model = "Neural Net")

combined_importance <- rbind(rf_imp, nn_wts) %>%
  group_by(Model) %>%
  mutate(Normalized_Importance = Importance / sum(Importance)) %>%
  ungroup()

top_features <- combined_importance %>%
  group_by(Feature) %>%
  summarize(Avg_Importance = mean(Normalized_Importance)) %>%
  arrange(desc(Avg_Importance)) %>%
  head(10) %>%
  pull(Feature)

plot_df <- combined_importance %>% filter(Feature %in% top_features)
ggplot(plot_df, aes(x = reorder(Feature, Normalized_Importance), y = Normalized_Importance, fill = Model)) +
  geom_bar(stat = "identity", position = "dodge") +
  coord_flip() +
  labs(title = "Top 10 Feature Importance", x = "Feature", y = "Normalized Importance") +
  theme_minimal()

```
**Compared Random Forest vs Neural Network feature importance. Top features: PR_POV, F_DIABETES, PR_NOHSDP, PR_ODW, PR_UNINSUR**


***9. Partial Dependence Plots (Top 5 RF features)***

```{r}
library(pdp)
top5_features <- rf_imp_df$Feature[1:5]

create_pdp <- function(feature, model, data) {
  pd <- partial(model, pred.var = feature, train = data)
  ggplot(pd, aes_string(x = feature, y = "yhat")) +
    geom_line(color = "steelblue", linewidth = 1.2) +
    geom_rug(sides = "b", alpha = 0.2) +
    labs(title = paste("Partial Dependence Plot for", feature),
         x = feature, y = "Partial Effect on Heat Vulnerability") +
    theme_minimal()
}

# Generate and plot PDPs
pdp_plots <- lapply(top5_features, create_pdp, model = rf_model, data = X_train)
do.call(grid.arrange, c(pdp_plots, ncol = 2))
```
**PDPs help us understand how a feature affects predictions, not just how important it is. Here, it showed how changes in each feature (e.g., poverty, uninsurance) affect the predicted probability of heat risk — based on Random Forest model.**
**Poverty, lack of education, and disability sharply increase risk below the 25% percentile, indicating threshold effects. Uninsurance and English proficiency show nonlinear trends — risk peaks at moderate levels. These plots make the model interpretable and guide where targeted interventions will be most impactful.**

***10. MAPPING TOP 20 Vulnerable ZIP CODES & RESEARCH ANALYSIS***
```{r}
# 9. MAPPING TOP 20 Vulnerable ZIP CODES
# Load required libraries
library(sf)
library(tigris)
library(ggplot2)
library(dplyr)

options(tigris_use_cache = TRUE)

# Load shapefile
zcta_shapes <- zctas(cb = TRUE, year = 2020)
names(zcta_shapes)[names(zcta_shapes) == "ZCTA5CE20"] <- "ZCTA"

# Generate predicted probabilities from Random Forest
rf_probs_all <- predict(rf_model, newdata = X, type = "prob")[, 2]
df$Predicted_Prob <- rf_probs_all

# Merge shapefile with data
zcta_ca <- zcta_shapes %>% filter(ZCTA %in% df$ZCTA)
map_data <- left_join(zcta_ca, df, by = "ZCTA")

# Plot heatmap
ggplot(map_data) +
  geom_sf(aes(fill = Predicted_Prob), color = NA) +
  scale_fill_viridis_c(name = "Heat Vulnerability", option = "inferno") +
  labs(
    title = "Heat Vulnerability Across California ZIP Codes",
    subtitle = "Predicted using Random Forest Model"
  ) +
  theme_minimal(base_size = 13) +
  theme(legend.position = "right")

# Identify top 20 ZIPs
top20_zips <- map_data %>%
  st_drop_geometry() %>%
  arrange(desc(Predicted_Prob)) %>%
  slice(1:20) %>%
  pull(ZCTA)

top20_map <- map_data %>% filter(ZCTA %in% top20_zips)

# Plot plain map with only top 20 ZIP codes highlighted
ggplot() +
  geom_sf(data = zcta_ca, fill = "grey90", color = "white", size = 0.05) +
  geom_sf(data = top20_map, fill = "firebrick", color = "black", size = 0.3) +
  labs(
    title = "Top 20 Most Heat-Vulnerable ZIP Codes in California",
    subtitle = "Based on Random Forest Predicted Probabilities"
  ) +
  theme_minimal(base_size = 13)
```
**Mapped predicted probabilities from Random Forest to CA ZIP codes. Highlighted the Top 20 most vulnerable ZIP codes on the map.**

***11. RESEARCH QUESTION ANALYSIS***

```{r}
# Combine final metrics
performance_metrics <- data.frame(
  Model = c("Random Forest", "Decision Tree", "Neural Network"),
  Accuracy = c(rf_conf$overall["Accuracy"], tree_conf$overall["Accuracy"], nn_conf$overall["Accuracy"]),
  Sensitivity = c(rf_conf$byClass["Sensitivity"], tree_conf$byClass["Sensitivity"], nn_conf$byClass["Sensitivity"]),
  Specificity = c(rf_conf$byClass["Specificity"], tree_conf$byClass["Specificity"], nn_conf$byClass["Specificity"]),
  F1_Score = c(rf_conf$byClass["F1"], tree_conf$byClass["F1"], nn_conf$byClass["F1"]),
  AUC = c(rf_auc, tree_auc, nn_auc)
)
print(performance_metrics)
```
**Random Forest clearly outperforms with the highest AUC, accuracy, and F1-score. It offers a strong balance between sensitivity (catching high-risk ZIPs) and specificity (avoiding false alarms).**
**While Neural Network captures complex patterns well, it lacks interpretability compared to Random Forest.**

***Q1: Prediction strength***
```{r}
prediction_analysis <- data.frame(
  Question = "Can we predict high-risk heat vulnerability zones using social, economic and demographic indicators in classification models?",
  Best_Model = performance_metrics$Model[which.max(performance_metrics$AUC)],
  Best_AUC = max(performance_metrics$AUC),
  Best_Accuracy = max(performance_metrics$Accuracy),
  Best_F1 = max(performance_metrics$F1_Score)
)
print(prediction_analysis)

# Plotting CA Map with Top 20 vulnerable ZIPs labelled and listed
install.packages("patchwork")
library(ggplot2)
library(dplyr)
library(sf)
library(tigris)
library(ggrepel)
library(gridExtra)
library(patchwork)  # for side-by-side layout

options(tigris_use_cache = TRUE)

# a. Load shapes and prepare prediction
zcta_shapes <- zctas(cb = TRUE, year = 2020)
names(zcta_shapes)[names(zcta_shapes) == "ZCTA5CE20"] <- "ZCTA"
zcta_ca <- zcta_shapes %>% filter(ZCTA %in% df$ZCTA)

df$Predicted_Prob <- predict(rf_model, newdata = X, type = "prob")[, 2]
map_data <- left_join(zcta_ca, df, by = "ZCTA")

# b. Top 20 ZIPs with rank
top20_df <- map_data %>%
  st_drop_geometry() %>%
  arrange(desc(Predicted_Prob)) %>%
  slice(1:20) %>%
  mutate(Rank = 1:20) %>%
  select(Rank, ZCTA, Predicted_Prob)

top20_zips <- top20_df$ZCTA
top20_map <- map_data %>% filter(ZCTA %in% top20_zips)
top20_map <- left_join(top20_map, top20_df[, c("ZCTA", "Rank")], by = "ZCTA")

# c. Centroids for labeling
top20_map_coords <- st_centroid(top20_map)
top20_coords <- cbind(top20_map_coords, st_coordinates(top20_map_coords))

# d. Map with labels
map_plot <- ggplot() +
  geom_sf(data = zcta_ca, fill = "grey90", color = "white", size = 0.05) +
  geom_sf(data = top20_map, fill = "firebrick", color = "black", size = 0.3) +
  geom_text_repel(data = top20_coords,
                  aes(x = X, y = Y, label = Rank),
                  size = 4, fontface = "bold", color = "white",
                  bg.color = "black", bg.r = 0.15,
                  box.padding = 0.2, point.padding = 0.3,
                  segment.color = "gray30") +
  labs(
    title = "Top 20 Most Heat-Vulnerable ZIP Codes in California",
    subtitle = "Ranked by Predicted Risk (Random Forest)"
  ) +
  theme_minimal(base_size = 13)

# e. Index Table as ggplot
table_plot <- ggplot(top20_df, aes(x = 1, y = reorder(ZCTA, -Rank))) +
  geom_text(aes(label = paste0(Rank, ".  ", ZCTA)), hjust = 0, size = 4.2) +
  xlim(1, 4) +
  theme_void(base_size = 13) +
  labs(title = "Index: ZIP Code Rankings") +
  theme(plot.title = element_text(hjust = 0.5))

# f. Combine map + table
final_plot <- map_plot + table_plot + plot_layout(widths = c(2.8, 1))
print(final_plot)
```
**Answer: YES — with high reliability and accuracy.**

**The Random Forest classifier excelled at distinguishing high-risk from low-risk ZIP codes with near-perfect AUC.**

**Even the simpler Decision Tree model performed well, demonstrating the predictive power of the selected features.**

**The models were trained on social (e.g., poverty, education), environmental (e.g., tree cover, pollution), and health (e.g., diabetes) indicators — which are meaningful and measurable.**

**ROC curves further confirm high model sensitivity and specificity.**

**Conclusion:**

**Yes, ZIP codes can be reliably classified by heat vulnerability using these indicators. Your models prove it with >90% accuracy and excellent AUC. This paves the way for targeted interventions and data-driven policy decisions.**

***Q2: Most important indicators of heat vulnerability***

```{r}
# Extract Random Forest feature importance
rf_top_indicators <- rf_imp_df %>%
arrange(desc(MeanDecreaseGini)) %>%
slice(1:5) %>%
rename(Importance = MeanDecreaseGini)
print(rf_top_indicators)

# Bar chart of top 5 predictors
ggplot(rf_top_indicators, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(
    title = "Top 5 Predictors of Heat Vulnerability (Random Forest)",
    x = "Feature",
    y = "Mean Decrease in Gini"
  ) +
  theme_minimal(base_size = 13)

```
**The top 5 predictors of heat vulnerability (from Random Forest):**

**PR_POV, F_DIABETES, PR_NOHSDP, PR_ODW, 	PR_UNINSUR **

**Insights from Feature Importance:**

**These features appeared consistently across Random Forest, Neural Network, and Decision Tree models — reinforcing their validity.**

**Partial dependence plots showed that increasing poverty, uninsurance, and low education sharply raise heat vulnerability scores.**

**Environmental indicators (e.g., impervious surfaces, low tree canopy) were also significant but ranked lower than social/health factors.**

**Conclusion:**
**The socioeconomic and health-related indicators were the most powerful drivers of heat vulnerability — not just environmental exposure. This highlights systemic inequality as a major factor and offers clear direction for policy interventions.**

***Q3.How can this inform targeted policy?***

**This project uses machine learning to identify ZIP codes in California that are most vulnerable to extreme heat based on socioeconomic, environmental, and health indicators. The results provide clear guidance for data-driven, targeted policy interventions, as explained below:**

**1. Geographically Targeted Action:**

**The model maps the top 20 most heat-vulnerable ZIP codes.**
**Policymakers can prioritize these ZIP codes for interventions such as Cooling centers**
**Heat emergency plans**
**Urban greening (e.g., planting trees to increase canopy)**
**Infrastructure upgrades (e.g., shade shelters, hydration points)**

**2. Addressing Social Inequity:**

**Key predictors like poverty, lack of health insurance, and low education levels suggest that social and economic inequality is closely tied to heat risk.**
**Programs should allocate more resources to low-income communities, especially:**
**Public health services**
**Utility bill subsidies for air conditioning**
**Community awareness campaigns in multiple languages**

**3. Healthcare-Focused Policy:**

**Health indicators like diabetes and asthma prevalence were among the top predictors.**
**Policies can integrate climate risk screening into local healthcare systems and target vulnerable individuals with:**
**Mobile health units during heat waves**
**Medication delivery and check-ins**
**Telehealth services in heat-prone areas**

**4. Climate-Resilient Urban Planning:**

**Environmental variables such as low tree canopy and high impervious surface areas increase heat exposure.**
**Urban planning policies should:**
**Promote green infrastructure**
**Restrict new developments in heat-prone zones unless mitigations are in place**
**Incentivize cool roofing and permeable pavements**

**5. Data-Driven Monitoring:**

**This model can be updated annually with new data to track changes in vulnerability.**
**State or city agencies can build early warning systems using model outputs to trigger alerts when certain ZIPs are forecast to face heat waves.**

**Final Takeaway:**

**By linking heat risk to measurable indicators, this model helps policymakers move from reactive to proactive intervention, ensuring that resources are distributed equitably and the most vulnerable communities are protected first.**

